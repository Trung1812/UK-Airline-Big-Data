1. create cluster (remove additional-component flag)

1.1. 1 master, 0 worker
gcloud dataproc clusters create cluster-streaming \
    --enable-component-gateway \
    --no-address --single-node \
    --region asia-northeast1 \
    --subnet default \
    --master-machine-type n2-standard-4 \
    --master-boot-disk-size 100GB \
    --image-version 2.2-debian12 \
    --optional-components JUPYTER \
    --metadata GCS_CONNECTOR_VERSION=2.2.2 \
    --metadata spark-bigquery-connector-version=0.21.0 \
    --bucket dataproc-staging-asia-southeast1-190992537547-yuriiara

1.2 1 master, 2 workers
gcloud dataproc clusters create streaming-cluster \
    --region asia-east2 \
    --zone asia-east2-a \
    --master-machine-type=n1-standard-4 \
    --master-boot-disk-size=100GB \
    --num-workers=2 \
    --worker-machine-type=n1-standard-4 \
    --worker-boot-disk-size=100GB \
    --image-version 2.2-debian12 \
    --subnet default \
    --enable-component-gateway \
    --bucket dataproc-staging-asia-southeast1-190992537547-yuriiara \
    --public-ip-address \
    --initialization-actions gs://uk-airline-big-data/scripts/initialization_actions.sh \
    --metadata spark-bigquery-connector-version=0.41.0

gcloud dataproc clusters update streaming-cluster \
    --region asia-east2 \
    --zone asia-east2-a \
    --initialization_action


2. Create VM instance
gcloud compute instances create kafka-instance --project=totemic-program-442307-i9 \
 --zone=asia-southeast1-a --machine-type=e2-medium \
 --network-interface=address=35.240.239.52,network-tier=PREMIUM,stack-type=IPV4_ONLY,subnet=default \
 --maintenance-policy=MIGRATE --provisioning-model=STANDARD \
 --service-account=bq-gcs-admin@totemic-program-442307-i9.iam.gserviceaccount.com \
 --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/trace.append \
 --create-disk=auto-delete=yes,boot=yes,device-name=kafka-instance,image=projects/debian-cloud/global/images/debian-12-bookworm-v20241112,mode=rw,size=10,type=pd-balanced \
 --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --labels=goog-ec-src=vm_add-gcloud --reservation-affinity=any

 gcloud compute instances describe kafka-instance \
    --zone=asia-southeast1-a \
    --format="get(disks.deviceName,disks.diskSizeGb,disks.type)"

3. Submit Jobs
gcloud dataproc jobs submit pyspark streaming.py \
    --cluster streaming-cluster \
    --region asia-east2 \
    --properties spark.submit.deployMode=cluster

spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --class com.example.YourMainClass \
  test_spark_kafka.py


